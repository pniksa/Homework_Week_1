{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Version (just read this part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some datapoints, we can model the data with a regression model.\n",
    "However, when choosing a regression model, one may under- or overfit the data.\n",
    "Two key quantities that determine the error of a model, alongside any intrinsic error or noise in the data itself is the variance and the bias of the model.\n",
    "Variance measure the spread of the fit function, while bias measures the deviation of the model from the true function that describes the data.\n",
    "Variance is typical small for low order polynomial fits, while the bias is often larger for very simplistic models e.g. low order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some datapoints, we can model the data with a regression model.\n",
    "However, when choosing a regression model, one may under- or overfit the data.\n",
    "Two key quantities that determine the error of a model, alongside any intrinsic error or noise in the data itself is the variance and the bias of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance:\n",
    "Given an expectation value $<f(D)>$, where D is a dataset and f is a model.\n",
    "The variance of the model for a given dataset corresponds to ${\\rm Var}(f)=<f^2(D)> - <f(D)>^2$.\n",
    "A high variance indicates overfitting as the model highly adjusts to variations in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is a measure of our assumptions about the expectation about how the dataset may be modeled best.\n",
    "Let us assume we know the perfect model $\\hat{f}$.\n",
    "Then the bias of the model corresponds to ${\\rm Bias}(f, \\hat{f})<\\hat{f}> - <f>$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the total error of a model can be approximated as intrinsic error + Var + Bias$^2$.\n",
    "In general we want to keep said error as low as possible.\n",
    "The intrinsic error is related to the lack of a perfect model in general and related to the incompleteness and variability of the dataset in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Point of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = [1, 2, 3, 4, 5, 6, 7]\n",
    "y = np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model exp and true model exp:\n",
    "In that case bias will vanish for this dataset obviously, but variance will not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var  137379.83489128752\n"
     ]
    }
   ],
   "source": [
    "#assumes \\hat{f} and f to be exponential\n",
    "Erx = 0\n",
    "Erx2 = 0\n",
    "for v in x:\n",
    "    Erx2 += np.exp(v) ** 2\n",
    "    Erx += np.exp(v)\n",
    "Erx2 /= len(x)\n",
    "Erx /= len(x)\n",
    "T1 = Erx2 - Erx ** 2\n",
    "print('Var ', Erx2 - Erx ** 2)\n",
    "#Bias vanishes\n",
    "#Var = Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other models bias will not vanish but variance may be smaller. Example $x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias  -64.86020575727244\n",
      "Variance  65416.98062232368\n",
      "Err  69623.8269131994\n",
      "Exp error over x^2 error:  1.973172705122344\n"
     ]
    }
   ],
   "source": [
    "#assumes \\hat{f} as exp(x) and f as a * x^2\n",
    "Er = 0\n",
    "Erx2 = 0\n",
    "Erx4 = 0\n",
    "#Cal coef a\n",
    "s1 = 0\n",
    "s2 = 0\n",
    "for v in x:\n",
    "    s1 += v ** 2 * np.exp(v)\n",
    "    s2 += v ** 4\n",
    "a = s1 / s2\n",
    "for v in x:\n",
    "    Er += np.exp(v)\n",
    "    Erx2 += v ** 2\n",
    "    Erx4 += v ** 4\n",
    "Er /= len(x)\n",
    "Erx2 /= len(x)\n",
    "Erx2 *= a\n",
    "Erx4 /= len(x)\n",
    "Erx4 *= a * a\n",
    "print('Bias ', Er - Erx2)\n",
    "print('Variance ', Erx4 - Erx2 ** 2)\n",
    "print('Err ', (Er - Erx2) ** 2 + Erx4 - Erx2 ** 2)\n",
    "T2 = (Er - Erx2) ** 2 + Erx4 - Erx2 ** 2\n",
    "print('Exp error over x^2 error: ', T1/T2)\n",
    "#1/(len(x))*(np.sum(pow(x, 2) ** 2) - 1/(len(x)) * np.sum(pow(x,2)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad example!\n",
    "\n",
    "Works for small x_i but does no longer work for several large x_i and becomes progressively worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
